\documentclass[letterpaper,12pt]{article}
%documentclass[superscriptaddress,preprintnumbers,amsmath,amssymb,aps,11pt]{revtex4}
%\usepackage[]{authblk}
%\usepackage{graphics}
\usepackage[dvipdf]{graphics}
%\usepackage{subfig}  % For subfloats
\usepackage{color}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{epsfig}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage{caption}
\usepackage{amsmath,amssymb}   % This was needed for "\text{\o}"
%\usepackage{subcaption}
\usepackage{subfig}
\usepackage{authblk}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    citecolor=red
}
%\usepackage{url}
\usepackage{appendix}

\usepackage{lineno}
\linenumbers

\oddsidemargin = -10mm
\topmargin = -2.cm
\textwidth = 18.5cm
\textheight = 23.5cm

\def \rarr {\rightarrow}
\def \grinp {\includegraphics}
\def \tw {\textwidth}
\def\dfrac#1#2{\displaystyle{{#1}\over{#2}}}
\def \dstl {\displaystyle}
\def \Mlr {M\o ller }
\def \Ap {$A^{\prime}$ }
\definecolor{GREEN}{rgb}{0.,0.8,0}
\definecolor{RED}{rgb}{1,0,0}
\definecolor{ORANGE}{rgb}{1,0.5,0}

\author{Bump hunt folks}
\title{Resonance search analysis of 2016 HPS spring run data.}

\begin{document}

\maketitle

\tableofcontents
\newpage

\section{Introduction}

The Heavy Photon Search (HPS) experiment has capability to search for a so called heavy photon \Ap (aka dark photon, U boson) with two complementary methods. 

\section{The final state of interest} \label{sec:FinalState}
Describe the Rad. Trident, BH, and converted WABs.


\section{Data set}
Describe the data, beam energy, beam current, target runs, etc.

\section{Event Selections}
This section describes all the cuts that are applied to get the final vertex candidate distribution. 
The main goal of event selection cuts is to maximize signal sensitivity.

In this analysis only events with ``Pair1'' trigger (see \cite{TriggerNote} for the description of HPS triggers) are used.

\subsection{Cluster timing cuts}\label{sec:cldTcuts}
The readout window of ECal FADC data is 200 ns. Clusters coming from the physics events, that generated the trigger, are located in a narrow time range (few ns width because of the trigger jitter) in the readout window around $\mathrm{t = 56\; ns}$.
%%%%%%%%%%%%%%%%%%%%%%%%%% F I G U R E %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!htb]
 \centering
 \subfloat{\label{fig:clE_t:Top}\grinp[width=0.45\tw]{Figs/cl_t_E_Top.pdf}}
 \subfloat{\label{fig:clE_t:Bot}\grinp[width=0.45\tw]{Figs/cl_t_E_Bot.pdf}}
 \caption{``time vs Energy'' distributions of ECal clusters in the Top (Left) and Bottom (Right) half. Red curves in the in the right plot indicate cuts that are applied to clusters in the bottom half for the initial cluster selection. See the text for the description of the difference between left and right plots.}
 \label{fig:cl_E_T_plots}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%% F I G U R E %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In Fig.\ref{fig:cl_E_T_plots} shown ``time vs Energy'' distributions of ECal clusters in the Top (Left) and Bottom (Right) halves. The bulge of events in the right plot are clusters that generated the trigger, and also trigger time is defined by these clusters. The noticeable energy dependence is due to the so called ``time walk Corrections'' \cite{ECalTimeCorr}. During initial event selection only clusters that are inside the outlined red curves are used, since the rest are accidentals that didn't come from the beam bunch generating the trigger. One can notice that for the clusters in the top half, in addition to the central bulge, there is an extra occupancy of events in region ($\mathrm{ 40\;ns < t_{cl} < 70\;ns }$). This is because the coincidence time between clusters in the ``Pair'' trigger was $\mathrm{12\;ns}$ \cite{TriggerNote}, and the trigger time is determined by the bottom cluster. Unlike to clusters in the bottom half, in the initial event selection, we have not cut on time of the top cluster, but rater we have applied cut on the cluster time difference between top and bottom clusters.
\subsubsection{Ad hoc ECal time corrections}
The next step is to cut pairs of top-bottom clusters that are far from each other in terms of time.
%%%%%%%%%%%%%%%%%%%%%%%%%% F I G U R E %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!htb]
 \centering
 \grinp[width=0.75\tw]{Figs/cl_dt_UncrAndCorr.pdf}
 \caption{Time difference between top and bottom clusters at high $\mathrm{E_{sum}}$ region. Dashed red lines show uncorrected clusters, and the blue curve show corrected clusters.}
 \label{fig:clTimeUncorCorr}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%% F I G U R E %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
During the analysis is was found that ECal cluster times can be improved, in particular the dashed red histogram in Fig.\ref{fig:clTimeUncorCorr} shows the time difference between top and bottom clusters\footnote{For the sake of better visualization, the plot doesn't fully show the entire central peak. } at high $\mathrm{E_{sum}}$ region ($\mathrm{1.9\;GeV  < E_{Sum} < 2.4\; GeV}$). As one can see there is a bump at around $\mathrm{2\;ns}$, while the at $\mathrm{-2\;ns}$ there is no clear bump. This suggests that time offsets of some crystals might be wrong.
%%%%%%%%%%%%%%%%%%%%%%%%%% F I G U R E %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!htb] 
 \centering
 \subfloat{\label{fig:timeCorr_clyxc}\grinp[width=0.95\tw]{Figs/ECal_YX_timeCorr.pdf}}\\
 \subfloat{\label{fig:timeCorrGrTop}\grinp[width=0.95\tw]{Figs/gr_TopCorrections.pdf}}\\
 \subfloat{\label{fig:timeCorrGrBot}\grinp[width=0.95\tw]{Figs/gr_BotCorrections.pdf}}\\
 \caption{Time Corrections for each crystal.}
 \label{fig:CrystalTimeCorr}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%% F I G U R E %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
To check this, for each crystal the time difference between that crystal and it's pair (in opposite half) crystal is constructed. The Top 2D plot of
Fig. \ref{fig:CrystalTimeCorr} shows mean values of each of crystals. 
The middle and the bottom plots show same mean values as a function of crystal $\mathrm{X}$ index for Top and Bottom crystal respectively. Different markers show different rows. 
As one can see crystals (-18, -1) and (-6, -3) are shifted from their immediate neighbors  by about 2 ns. There are some other crystals which are shifted significantly too (but less than 2ns). These include for example crystals (-16, -5), (10, -5), (10, 1) and (10, 2). In addition to this, we see that there is a general crystal X index (and slightly Y index) dependence too. In reality crystal X index is correlated to the charged particle energy too, and the original dependence might be not on X but on energy. Studying it is out of the scope of this note, and here for each crystal we have corrected the time, by subtracting these calculated mean values from the reconstructed cluster time.
After the correction the cluster time difference is depicted by blue solid line in Fig.\ref{fig:clTimeUncorCorr}. One can see that the excess of events at 2 ns disappeared. Dips and peaks between bumps indicating difference beam bunches also got sharper, which is an indication of an improvement of the cluster time resolution.

\subsubsection{Fitting Cluster time difference}
After correction of individual cluster times, the Top-Bottom cluster time difference was fitted with a following function:
\begin{equation}
 \displaystyle \mathrm{F = \sum_{i = 0}^{N_{peak}} a_{i}\cdot\left(Gaus(x - \mu^{1}_{i}, \sigma^{1}_{i}) + b\cdot Gaus(x - \mu^{2}_{i}, \sigma^{2}_{i})  \right)}
\end{equation}
where $\mathrm{N_{peak}}$ is the number of peaks. Each peak is described by the sum of two Gaussian functions $\mathrm{Gaus(x - \mu^{1}_{i}, \sigma^{1}_{i})}$ and $\mathrm{Gaus(x - \mu^{2}_{i}, \sigma^{2}_{i})}$ with their amplitude ratio ``b'.
The parameter ''b`` is the same for all peaks. 
In the fit, free parameters are $\mathrm{a_{i},\; \mu^{1}_{i},\; \sigma^{1}_{i},\; \mu^{2}_{i},\; \sigma^{2}_{i},\; b }$. 

The fit result is shown in Fig.\ref{fig:cldTFits}. Different peak components of the
%%%%%%%%%%%%%%%%%%%%%%%%%% F I G U R E %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!htb]
 \centering
 \subfloat[]{\label{fig:cldTFitLin1}\grinp[width=0.32\tw]{Figs/Calo_dt_Fit_lin1.pdf}}
 \subfloat[]{\label{fig:cldTFitLog1}\grinp[width=0.32\tw]{Figs/Calo_dt_Fit_log1.pdf}}
 \subfloat[]{\label{fig:cldTFitZoom1}\grinp[width=0.32\tw]{Figs/Calo_dt_Fit_zoom1.pdf}}
 \caption{Fit of the Top and Bottom cluster time difference. Left: linear scale, Middle: Log scale and right: Linear scale but includes only low magnitude peaks. Main Gaussian functions are represented by solid lines, and the secondary Gaussian (with wider width and lower magnitude) are represented by dashed lines.}
 \label{fig:cldTFits}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%% F I G U R E %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
function are depicted by different color. The main Gaussian function of each peak is represented by a solid line, while the secondary Gaussian is represented with a dashed line. One can see that this function fits the distribution reasonably well.

Then in order to determine the optimal cut on the cluster time difference, we will use the value, which maximizes the $\mathrm{\frac{\dstl S}{\dstl \sqrt{S + Bgr}}}$ ratio, where ''S`` is the signal (in our case the central peak), and ''S + Bgr`` is the signal plus Background (the total fit function).
%%%%%%%%%%%%%%%%%%%%%%%%%% F I G U R E %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!htb]
 \centering
 \grinp[width=0.75\tw]{Figs/dt_cur_optimize1_zoom.pdf}
 \caption{The $\mathrm{\frac{\displaystyle S}{\displaystyle \sqrt{S + Bgr}}} $ ratio as a function of cluster time difference cut. The Dashed line indicates the maximum of the function.}
 \label{fig:cl_dtOptimumCut}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%% F I G U R E %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The $\mathrm{\frac{\displaystyle S}{\displaystyle \sqrt{S + Bgr}}} $ ratio as a function of cluster time difference cut is shown in Fig.\ref{fig:cl_dtOptimumCut}, where the maximum value at $\mathrm{\Delta t < 1.43\;ns}$ is indicated by a vertical dashed line.
\clearpage
\subsection{Anti-FEE cut}
The purpose of this cut is to eliminate elastically scattered electrons.
In principle if we don't put any cut, then an elastically scattered electron (aka FEE) can be in accidental  coincidence with a positron in the opposite half, and that electron-positron pair would look like a trident candidate.
%%%%%%%%%%%%%%%%%%%%%%%%%% F I G U R E %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!htb]
 \centering
\subfloat{\label{fig:FEECutLeft}\grinp[width=0.45\tw]{Figs/FEE_Cut_AllBut.pdf} } 
\subfloat{\label{fig:FEECutZoom}\grinp[width=0.45\tw]{Figs/FEE_CutAllButZoom.pdf} }
\caption{Electron momentum distributions obtained representing Data (Black points), 
Trident (Red points), and Raditative tridents (blue points). The vertical red line represents the cut.
The right plot is the same distribution but zoomed that the tail of FEEs can be visible.}
\label{fig:FEECut}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%% F I G U R E %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In Fig.\ref{fig:FEECut} shown electron momentum distributions representing Data (Black points), 
Trident (Red points), and Raditative tridents (blue points). All these distributions passed all event selection cuts except the electron maximum momentum cut. Left and right figures are the same,
the only difference is that in the right figure the vertical axis spans only small values region,
where one can see the behavior of the high energy tail of the distribution. As one can see, when the rest of event selection cuts are applied, then there is no hint of the FEE that potentially could be misidentified as a trident electron. The cut is chosen such to keep 99.9\% electrons from the given sample.
The vertical red line represents the cut for MC samples, while the black line represents the cut value for data.
\clearpage
\subsection{\texorpdfstring{$\mathrm{P_{Sum}}$}{Lg} Min cut} 
As it is explained in the introduction and in \cite{AprimeFixedTargetTheory}, \Ap and Radiative tridents have identical kinematics, while they are quite different from BH. We need to chose a phase space, where radiative tridents (signal) are maximized while Bgr (BH and converted WABs) are minimal.
Unlike BH, radiative tridents are peaked at higher $\mathrm{P_{Sum}}$, so we should chose a high $\mathrm{P_{Sum}}$ region.
The Figure of Merit (FOM) is chosen as $\frac{\dstl \mathrm{S}}{\dstl \sqrt{\mathrm{Tot}}}$, where $\mathrm{S}$ is the number of radiative trident events, while ``$\mathrm{Tot}$'' is the total number of observed events. In the data we of course don't have a way to know whether the given event is radtiative trident, BH or converted WAB event, and that is why we have to relay on MC in order to chose the $\mathrm{P_{Sum}}$ minimum momentum cut that maximize the FOM.
%%%%%%%%%%%%%%%%%%%%%%%%%% F I G U R E %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!htb]
 \centering
 \subfloat{\label{fig:PSUMCComponents}\grinp[width=0.45\tw]{Figs/PSum3_MC_Components.pdf}}
\subfloat{\label{fig:PSUMFOM}\grinp[width=0.45\tw]{Figs/PSum3_FOM.pdf}}
\caption{Left plot represents $\mathrm{P_{Sum}}$ distribution for different MC components, while the right plot represents the FOM as a function of the $\mathrm{P_{Sum}}$ Min cut. The vertical line on the right plot shows the cut value, whre the FOM is maximum.}
\label{fig:PSUMMINFOM}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%% F I G U R E %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In the left plot of Fig.\ref{fig:PSUMMINFOM} shown difference MC components: Wab (brown), Rad (red), Tri (blue) and Wab + Tri (orange), when the rest of event selection cuts are applied. The right plot shows the FOM as a function of the $\mathrm{P_{Sum}}$ cut. The $\mathrm{P_{Sum}}$ value that maximizes the FOM is chosen for the $\mathrm{P_{Sum}}$ Min cut.
\clearpage
\subsection{WAB Suppression cuts}
In the section \ref{sec:FinalState} it is described that, the two step process cWAB $eA\rarr eA\gamma(\rarr e^{-}e^{+})$ can mimic the trident final sate. As the photon conversion can occur in the target, it also can occur in any layer of SVT too. While the photon conversion in layers 3, 4, 5 and 6 will not yield to reconstructed trucks (since tracking requires at least 5 3D hits), conversion in the first and second layer (if conversion occur in the axial sensor of layer 2) could cause both or either $e^{-}$ or $e^{+}$ to be reconstructed. One of handles to suppress cWABs, is to require a presence of a hit in L1 from $e^{+}$ tracks. This should suppress cWABs significantly (photon conversions in L2 and in L1 stereo sensor), while should not significantly impact tridents that produced in the target. In this particular analysis, however, we will not require the presence of a hit in L1, and the reason is that at this moment we have significant discrepancy in the Data and MC L1 hit efficiency, which in turn translates to more discrepancy between data and MC rates.

\subsubsection{\texorpdfstring{$\mathrm{d_{0} }$}{Lg} cut}
Another strong cut that allows to suppress cWABs is the cut on the distance of closest approach ($\mathrm{d_{0}}$). The reason is that, if the photon conversion occurs not in the target, but in 
%%%%%%%%%%%%%%%%%%%%%%%%%% F I G U R E %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!htb]
 \centering
 \grinp[width=0.65\tw]{Figs/cWABDiagram.png}
 \caption{A sketch diagram representing a photon conversion in the 2nd layer of SVT. It also demonstrates that the reconstructed trajectory will be displaced from the photon production point.}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%% F I G U R E %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
any layer of SVT, then the reconstructed trajectory will not pass through the beam-target interaction point, but rather will be systematically displaced from the interaction point in the target. This signature is one of important differences between the $eA\rarr A e^{-}e^{+}e^{-}$ process and the two step process 1st: $eA\rarr A e \gamma$ then 2nd $\gamma \rarr e^{-}e^{+}$. The cut value is determined using both, the data and MC simulations. The objective is to chose a cut value, that will maximize the reach of the experiment. Similarly to several of other cuts, the FOM is chosen as
\begin{equation}
 \mathrm{FOM = \frac{S}{Tot}}
 \label{eq:FOM_D0}
\end{equation}
where ``$\mathrm{S}$'' is the number of Radiative events, and ``Tot'' is the total number of events, which is the sum of Rad. tridents, BH (including interference term between them) and cWABs. In data we don't know which of the abovementioned processes is responsible for the given event. Instead we will use the MC simulations for all abovementioned components.
%%%%%%%%%%%%%%%%%%%%%%%%%% F I G U R E %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!htb]
 \centering
 \subfloat[]{\label{fig:d0MCComponents}\grinp[width=0.45\tw]{Figs/d0_MC_Components.pdf}}
 \subfloat[]{\label{fig:d0FOM}\grinp[width=0.45\tw]{Figs/d0_FOM_MC.pdf}}
 \caption{Left: Observed cross differential cross section as a function of
 $\mathrm{d_{0}}$ for different MC components (described in the text and in the figure). 
 Right FOM as a function of $\mathrm{d_{0}}$ cut value.}
 \label{fig:d0MC_ANDFOM}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%% F I G U R E %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In Fig.\ref{fig:d0MCComponents} shown differential cross section $\mathrm{\dfrac{\dstl d\sigma}{\dstl dd_{0}}}$ for all MC components as a function of $\mathrm{d_{0}}$. As one can note from the Fig.\ref{fig:d0MCComponents}, $\mathrm{d_{0}}$ is peaked at 0 for Rad and Tridents, but for cWABs the peak is shifted, and has a quite long tail on the positive $\mathrm{d_{0}}$ side. The Fig.\ref{fig:d0FOM} shows the FOM defined by eq.\ref{eq:FOM_D0} as a function of $\mathrm{d_{0}}$ cut value.
%%%%%%%%%%%%%%%%%%%%%%%%%% F I G U R E %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!htb]
 \centering
 \subfloat[]{\label{fig:d0FitMC}\grinp[width=0.45\tw]{Figs/d0_MC_Fit.pdf}}
 \subfloat[]{\label{fig:d0FitData}\grinp[width=0.45\tw]{Figs/d0_Data_Fit.pdf}}
 \caption{differential cross section as a function of $\mathrm{d_{0}}$. Left figure represents MC data (Tritrig + cWAB), while right figure represents the data.}
 \label{fig:d0Fits}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%% F I G U R E %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 This value obtained by maximizing the FOM using only the MC will not represent the FOM for the data sample, because the $\mathrm{d_{0}}$ resolution in Data and MC are significantly different. As one
 can see from the Fig.\ref{fig:d0Fits}, the $\mathrm{d_{0}}$ resolution in data is about 1.6 times wider than the one for MC. Instead for the data sample, the $\mathrm{d_{0}}$ cut is chosen in a way,
 that the distance between the cut value and the main peak is the same in the units of the $\mathrm{d_{0}}$ resolution ($\mathrm{\sigma_{d_{0}}}$). So in data the $\mathrm{d_{0}}$ cut value is
 defined as:
 \begin{equation}
  \mathrm{\displaystyle {d_{0}}^{cut}(Data) = \mu_{d_{0}}(Data) + \sigma_{d_{0}}(Data)\frac{\displaystyle {d_{0}}^{cut}(MC) - \mu_{d_{0}}(MC) }{\displaystyle \sigma_{d_{0}}(MC)} }
 \end{equation}
Mean, $\sigma$ and cut value on $d_{0}$ for data and MC are shown in the table.\ref{tb:d0Values}.
 
 \begin{table}[!htb]
 \centering
  \begin{tabular}{|l|c|c|}
   \hline
                                    & MC            & Data              \\ \hline
 mean of $\mathrm{d_{0}}$ [mm]      & -0.021        & 0.141             \\ \hline
 $\sigma$ of $\mathrm{d_{0}}$ [mm]  & 0.296         & 0.484             \\ \hline
 $\mathrm{d_{0}}$ cut value [mm]    & 0.51          & 1.011             \\ \hline
  \end{tabular}
  \caption{Mean, $\sigma$ and cut value on $\mathrm{d_{0}}$ for data and MC.}
\label{tb:d0Values}
 \end{table}

 
\subsection{Two dimensional cuts}
Some of event selections cuts described below are two dimensional cuts, i.e. the cut value depends on the value of another variable. In most of cases two dimensional cuts are implemented as a function of particle's momentum.

In general, to study the distribution of a given variable for a ''signal like`` particle, the rest of event selection cuts are applied, to make as clean as possible signal.  
The only exception is the two cluster time difference cut, which is described in section \ref{sec:cldTcuts} ).
Applying the rest of cuts except the one under the investigation, will ensure the 
accidental background is minimal (negligible), and the resulting distribution will represent actuall signal (the $e^{-}$, $e^{+}$, ($X$) final state). In most of cases the distribution is not Gaussian, even when it represent a small momentum bin.
%%%%%%%%%%%%%%%%%%%%%%%%%% F I G U R E %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!htb]
 \centering
 \grinp[width=0.75\tw]{Figs/CutLimitTests.pdf}
 \caption{Illustration of  $3\sigma$ cut limits vs $99\%$ cut limits on a toy distribution.}
 \label{fig:CutLimitIllustration}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%% F I G U R E %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In such cases, the conventional $\pm 3\sigma$ cuts will not keep $99.7\%$ but rather  might cut more events. As an example in Fig.\ref{fig:CutLimitIllustration} shown a toy distribution which is not a pure Gaussian, but rather has a tail on the left side. The Gaussian fit is shown on top if the histogram and $\pm 3\sigma$ limits are shown by vertical red lines. One can see that $-3\sigma$ limit will cut several $\%$ of events rather than $0.3\%$. Instead it was decided to choose left and right cuts limits such that will keep $99\%$ of the signal and will throw $0.5\%$ of signal events from each side. In this particular case $99\%$ cut limits are shown by blue vertical lines. \newline \indent
There are some special cases in this algorithm, which are explained below. 
\vskip 1cm
1. The number of events in the one dimensional projected histogram is too low. In this case, when then number of events is below 45, then the entire momentum bin is considered as out of acceptance. Note: the data sample that we use, has such statistics, that two dimensional original histograms that we use have about 200K or more events, and binning is chosen such, that the total number of events that are in the ''$N < 45$`` category one dimensional histograms are significantly less than $1\%$ of the original two dimensional histogram.
\vskip 1cm
2. The number of events in the one dimensional histogram is more than 45, however it is not high enough (order of thousands). In this case the $0.5\%$ events that should be cut out from each side of the distribution is very small number e.g. 1, 2, 3 or even 0, if the number of events is in between 45 and 200. So in order to apply some cuts rather than no-cut or very loose cut, the $99\%$ requirement is released for histograms having less than 500 events ($N < 500$).
In general cut limits for different statistic cases are summarized
%%%%%%%%%%%%%%%%%%%%%%%%%% T A B L E %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[!htb]
 \centering
 \begin{tabular}{|l|r|}
  \hline
 \textbf{ $\#$ of events} & \textbf{Cut limit} \\ \hline
  $\mathrm{ N > 500}$ & $99\%$ \\ \hline
  $\mathrm{200 < N < 500}$ & $98\%$ \\ \hline
  $\mathrm{100 < N < 200}$ & $96\%$ \\ \hline
  $\mathrm{45 < N < 100}$ & $95\%$ \\ \hline
  $\mathrm{N < 45}$ & $0\%$ \\ \hline
 \end{tabular}
\caption{Cut limits for different statistic scenarios.}
\label{tb:2DCutLimits}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%% T A B L E %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
in table \ref{tb:2DCutLimits}.

\subsection{Track-Cluster Matching}
The offline reconstruction code forms particles by matching tracks and clusters to each other, by utilizing spatial coordinate and time differences between tracks and clusters.
In the offline reconstruction the matching is quite loose (''Better to keep junk, rather than throwing a good particle``). In this section spatial and time matching cuts are described. Both, time and position resolution of $e^{-}$ and $e^{+}$ clusters depend on particle momentum. The precision of the track projected coordinate at the ECal face does depend on the track momentum too. Because of these reasons we studied track-cluster matching as a function of momentum.

\subsubsection{time matching}
In addition to the momentum dependence we noticed also slight difference between top and bottom sectors, therefore two separate cuts are developed for each detector half.
% %%%%%%%%%%%%%%%%%%%%%%%%%% F I G U R E %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!htb]
 \centering
 \subfloat[]{\label{fig:ClTrkDtBot}\grinp[width=0.45\tw]{Figs/trk_cl_timeDiffCut_Bot_Data.pdf}}
 \subfloat[]{\label{fig:ClTrkDtTop}\grinp[width=0.45\tw]{Figs/trk_cl_timeDiffCut_Top_Data.pdf}}
 \caption{Cluster-Track time difference as a function of particle momentum. Left plot represents particles in the bottom half, and the right plot represents particles in the top half of the detector. The area marked by Red dots represents the acceptance region. }
 \label{fig:ClTrkTimeMatch}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%% F I G U R E %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In Fig.\ref{fig:ClTrkTimeMatch} shown Cluster-Track time difference as a function of particle momentum. Left plot represents particles in the bottom half, and the right plot represents particles in the top half of the detector. 
{\color{Red} The Cl-Trk time difference plot should be replaced, i.e. The original 2D distribution here doesn't correspond to the ''n-1`` cut. }


\subsection{Track quality cuts}
The point of track quality cuts is to maximize the Figure Of Merit (FOM). 
It is natural to think, if the track quality (in terms of $\chi^{2}$ per degrees of freedom)
is poor, then that could result in worse mass resolution, 
and consequently will have a negative impact on the experiment reach. 

To maximize the the reach, we should maximize the FOM, which is 
\begin{equation}
 \mathrm{FOM} \sim \frac{\dstl \sqrt{N_{Tot}}}{\dstl \sigma_{m}}
 \label{eq:FOM_massres_Final}
\end{equation}
Where $\dstl \sqrt{N_{Tot}}$ is the number of events in the given mass bin, and 
$\dstl \sigma_{m}$ is the mass resolution for the given mass (see appendix \ref{sec:AppendixFOM} for more details about eq. \ref{eq:FOM_massres_Final}).

We have used the \Mlr process to estimate the impact of the track quality on the mass resolution. The square root of the center of mass energy in the \Mlr process, is fixed for a given beam energy, and is equal (neglecting electron mass square terms):
\begin{equation}
 \dstl M_{ee}^{\mathrm{c.m.}} = \sqrt{2\cdot m_{e}\cdot E_{b}}
\end{equation}
The square root of the center of mass energy is also equal to the invariant mass of final state electrons in the \Mlr process. Hence we will use the \Mlr process to estimate the effect of track quality on the mass resolution. As a measure of quality of track pairs, the  combined $\chi^{2}_{\mathrm{Sum}}/\mathrm{NDF_{Sum}}$ was used, which is defined as:
\begin{equation}
    \chi^{2}_{\mathrm{Sum}}/\mathrm{NDF_{Sum}} = \frac{\dstl \chi^{2}_{ \mathrm{Bot}} + \chi^{2}_{\mathrm{Top}} }{ \mathrm{\dstl 2\left(N_{Bot}^{hits} + N_{Top}^{hits}\right) - 10}}
\end{equation}
Here $\mathrm{\chi^{2}_{Bot(Top)}}$ is the $\chi^{2}$ for bottom (top) track, and 
$\mathrm{ N^{hits}_{Bot(Top) } }$ is the number of 3d hits for the bottom (top) track.
In the denominator $10$ is the total number of constraints (5 from each track).

\subsubsection{Selection of \Mlr events}
{\color{Red} Important: We need MC data to justify some of cuts. Some of them are natural, however they should be confirmed by MC.}

As a starting point, we have used so called ''\Mlr candidate events`` defined by the so called ''MOUSE`` cuts \cite{MOUSE_Cuts}. Those are events which contain at least one negative track in each detector half. The magnitude of their momentum sum also should be within $\mathrm{20\%}$ of beam energy:
%%%%%%%%%%%%%%%%%%%%%%%%%% E Q U A T I O N %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
 0.8E_{\mathrm{b}} < P_{\mathrm{M\text{\o} eller}} \equiv |\vec{P}_{\mathrm{Bot}} + \vec{P}_{\mathrm{Top}}| < 1.2E_{\mathrm{b}}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%% E Q U A T I O N %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Note: here there is no requirement for a track to be associated with a cluster. In the \Mlr selection of 2015 run, it was required both electrons to have a cluster, however about $\times 2$ higher beam energy of 2016 run boosts electrons more, and it is almost impossible to get both electrons to hit the Calorimeter (The ECal hole significantly reduces \Mlr acceptance), however there is non zero acceptance in the tracker for \Mlr electrons.
In \Mlr event selection, it is not critical to keep track on how much signal and background each cut throws, but rather it is important to have the final sample as clean as possible (event if some cuts might be tight). During the analysis the following cuts were used:
\begin{itemize}
 \item $\mathrm{P_{sum}}$ momentum sum of final state electrons. \Mlr kinematics requires this to be equal to the beam energy. In the analysis we will chose a region where the  $\mathrm{P_{sum}}$ is in the vicinity of beam energy.
 \item $\Delta t_{\mathrm{tr}}$ time difference between top and bottom tracks. Cutting $\Delta t_{\mathrm{tr}}$ around 0, will suppress accidentals coming from different beam bunches.
 \item Track-Cluster matching. In 2016 kinematics One of electrons misses the calorimeter, and, in the offline reconstruction, if both tracks are associated with a cluster, then it is very likely one of them is accidental or track and cluster are produced from different particles, and it will will yield a bad matching $\chi^{2}$ value (large value). To suppress accidentals we will require one of tracks to have a good matching $\chi^{2}$ (small value), and the other to have a poor $\chi^{2}$ (large value).
 \item Cut on $\mathrm{P_{diff}}$: Momentum difference between top and bottom electrons.
 This is cut is also based on the specific acceptance of \Mlr events. and certain momentum configuration enter into the detector acceptance. This also will help to suppress acceptance.
 \item $\mathrm{d\phi}$: Azimuthal angular difference of two final state electrons wrt beam direction at the target. In the \Mlr kinematics $\mathrm{d\phi}$ should be $180^{\circ}$. This cut however is not used. It cuts almost nothing, when the rest of cuts is applied.
\end{itemize}

Similarly to the rest of event selection cuts for the trident final state, here for \Mlr event selection also, in order to understand where to put cuts on a given variable, cuts are applied to the rest of variables mentioned above, then the distribution of the given variable is studied.
%%%%%%%%%%%%%%%%%%%%%%%%%% F I G U R E %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!htb]
 \centering
 \subfloat[]{\label{fig:MlrTrDt}\grinp[width=0.45\tw]{Figs/dT_tr5_Data.pdf}}
 \subfloat[]{\label{fig:MlrPsuPDiff}\grinp[width=0.45\tw]{Figs/PSumDiff3_Data.pdf}}
 \caption{Left: Time difference between top and bottom tracks. Right: Momentum difference vs Momentum sum of Top and bottom tracks. Red lines represent cut limits. Each of this distributions are made when cuts were applied to the rest of \Mlr selection variables. }
 \label{fig:Mlr_dT_and_PSumPDif}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%% F I G U R E %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In Fig.\ref{fig:Mlr_dT_and_PSumPDif} shown Time difference between top and bottom tracks (left) and Momentum difference ($\mathrm{P_{Diff}}$) vs Momentum sum ($\mathrm{P_{Sum}}$) of Top and bottom tracks (right). Red lines indicate cut region for corresponding variables {\color{Red}\textbf{ Again, we need proper MC justify these cuts }}.


%%%%%%%%%%%%%%%%%%%%%%%%%% F I G U R E %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!htb]
 \centering
 \subfloat[]{\label{fig:chi2VsMassMlr}\grinp[width=0.32\tw]{Figs/Chi2NDF_Mass_Moeller_Data.pdf}}
 \subfloat[]{\label{fig:MlrSigmChi2Cut}\grinp[width=0.32\tw]{Figs/MassSigmaChi2Cut_Data.pdf}}
  \subfloat[]{\label{fig:MlrChi2FOM}\grinp[width=0.32\tw]{Figs/MoellerFigOfMerit_Data.pdf}}

\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%% F I G U R E %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Parametrization of Mass resolution.}

\section{Bump hunt analysis}


\section{Study of systematics}
Here goes studies on systematics.




\appendix
\appendixpage
\addappheadtotoc
\section{Figure of Merit in terms of Mass resolution}
\label{sec:AppendixFOM}
In general, the sensitivity for a signal (in our case a dark photon $A^{\prime}$) which is expressed in a form of a peak over a continuous background, is proportional to the number of signal events $\dstl N_{A^{\prime}}$, and inversely proportional to the statistical uncertainty $\sigma_{\mathrm{stat}}$ of the distribution under the peak.
So the figure of merit is expressed as:
\begin{equation}
 \mathrm{FOM} = \frac{\dstl N_{A^{\prime}}}{\dstl \sigma_{\mathrm{stat}}}
 \label{eq:FOM_massres}
\end{equation}

The $\sigma_{\mathrm{stat}} = \sqrt{N_{\mathrm{Tot}}}$, and $N_{\mathrm{Tot}}$ is the total measured number of events in the given mass bin.

For a given $A^{\prime}$ mass, the expected number of dark photons, 
$\dstl N_{A^{\prime}}$ events in the given mass bin  can be expressed in terms of number of expected Radiative trident events $N_{\mathrm{Rad}}$ using the eq.(19) of \cite{AprimeFixedTargetTheory}:
\begin{equation}
 N_{A^{\prime}} = \left(\frac{\dstl 3\pi \epsilon^{2}}{\dstl 2 N_{f} \alpha}\right) \left( \frac{\dstl m_{A^{\prime}}}{\delta m} \right)\cdot N_{\mathrm{Rad}} = \left(\frac{\dstl 3\pi \epsilon^{2}}{\dstl 2 N_{f} \alpha}\right) \left( \frac{\dstl m_{A^{\prime}}}{\delta m} \right) \cdot N_{\mathrm{Tot}}\cdot f_{\mathrm{Rad}}
 \label{eq:NAprime}
\end{equation}
Here $f_{\mathrm{Rad}}$ is the radiative fraction, and $\delta m$ is the width of the mass bin which is proportional to the mass resolution $\sim \sigma_{m}$ (look \cite{AprimeFixedTargetTheory} for the description of the rest of variables). 
Using eq.\ref{eq:NAprime} for $N_{A^{\prime}}$, $\dstl\sqrt{N_{\mathrm{Tot}}}$ for 
$\sigma_{\mathrm{stat}}$, and $\dstl \sigma_{m}$ for $\delta m$, we can express FOM as
\begin{equation}
 \mathrm {FOM} \sim \frac{\dstl \sqrt{N_{\mathrm{Tot}}}}{\sigma_{mass}}
\end{equation}


\begin{thebibliography}{55}
 \bibitem{TriggerNote} Kyle McCarty, Valery Kubarovsky and Benjamin Raydo, ``Description and Tuning of the HPS Trigger'', HPS Note 2018-002
 \bibitem{ECalTimeCorr} Holly Szumila-Vance, ``HPS Ecal Timing Calibration for the
Spring 2015 Engineering Run'', HPS Note 2015-011.

\bibitem{AprimeFixedTargetTheory} J. D. Bjorken, R. Essig, P. Schuster and N. Toro,
``New Fixed-Target Experiments to Search for Dark Gauge Forces'', Phys.Rev. D80 (2009) 075018, \href{https://arxiv.org/abs/0906.0580}{arXiv:0906.0580}
\bibitem{MOUSE_Cuts} Miriam Diamond's talk at analysis group meeting On 16-Oct-2018, (password protected) \href{https://confluence.slac.stanford.edu/display/hpsg/October+16%2C+2018}{https://confluence.slac.stanford.edu/display/hpsg/October+16\%2C+2018}
\end{thebibliography}

\end{document}
